{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Neural Networks: A Foundational Introduction\n",
        "\n",
        "This lecture introduces neural networks, focusing on the fundamental concepts and building blocks.  We'll cover perceptrons, multi-layer perceptrons (MLPs), activation functions, backpropagation, and training. This foundation will be crucial for understanding more advanced topics like LLMs, Generative AI, Computer Vision, and Reinforcement Learning.\n",
        "\n",
        "**Prerequisites:** Familiarity with machine learning and supervised learning concepts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction and Motivation\n",
        "\n",
        "*   **Recap of supervised learning**\n",
        "*   **Neural networks** \n",
        "*   **Deep Learning Revolution**\n",
        "*   **Future Topics**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Supervised Learning Review\n",
        "\n",
        "As you already know, supervised learning is a fundamental branch of machine learning where we train a model on labeled data, meaning data with both inputs and desired outputs.  Our goal is to learn a mapping function that can accurately predict the output for new, unseen inputs. We've explored various algorithms like linear regression, logistic regression, support vector machines, and decision trees.  These methods have proven useful for many tasks, but they often rely heavily on feature engineering. This means we, as humans, need to carefully craft and select the right features from the raw data to feed into the model.  This process can be time-consuming, require domain expertise, and it's not always clear which features will be most effective. Furthermore, some algorithms struggle with highly complex, non-linear relationships in the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Neural Networks\n",
        "\n",
        "Now, let's turn our attention to neural networks.  Neural networks offer a powerful and flexible alternative. They are inspired by the structure and function of the human brain, although they are, of course, vastly simplified. At their core, neural networks are function approximators.  Given some input, they learn to produce an output.  But unlike the algorithms we've seen before, neural networks have the remarkable ability to learn complex, non-linear relationships directly from the data without the need for explicit feature engineering.  They achieve this through interconnected layers of artificial neurons, allowing them to automatically discover and extract relevant features. This ability to learn hierarchical representations makes them incredibly versatile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Learning\n",
        "Over the past decade, we've witnessed what's often called the 'deep learning revolution.' Deep learning, which refers to neural networks with multiple layers (hence 'deep'), has achieved groundbreaking results in a wide range of fields. Think about image recognition: self-driving cars, facial recognition, medical image analysis – all powered by deep learning. Natural language processing has also seen tremendous progress. We now have sophisticated chatbots, machine translation systems, and sentiment analysis tools, all thanks to deep learning. These are just a couple of examples.  Deep learning is transforming fields like robotics, drug discovery, finance, and many others."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Future Topics\n",
        "\n",
        "The power of neural networks extends far beyond the examples I just mentioned. And, importantly for you, they form the bedrock for many of the topics you'll be exploring later in this course. Large Language Models (LLMs), like the ones powering advanced chatbots, are built upon neural network architectures.\n",
        "\n",
        "Generative AI (GenAI), which allows us to create realistic images, text, and even music, relies heavily on specialized neural networks.  Computer vision, the field that enables computers to 'see' and interpret images, uses convolutional neural networks. And even in reinforcement learning, where agents learn to make decisions through trial and error, neural networks are often used to approximate the optimal policy. So, understanding the fundamentals of neural networks that we'll cover today is absolutely essential for your future studies in these cutting-edge areas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perceptron and Multi-Layer Perceptron (MLP)\n",
        "\n",
        "A perceptron is the simplest unit of a neural network. It takes multiple inputs, each multiplied by a weight, and sums them up. This sum is then passed through an activation function to produce an output. Think of it like a single neuron in your brain making a decision based on the signals it receives.\n",
        "\n",
        "Let's break down its components:\n",
        "\n",
        "* Inputs: These are the initial pieces of information fed into the perceptron, analogous to the signals a biological neuron receives through its dendrites.\n",
        "\n",
        "* Weights: Each input is associated with a weight, which represents the strength or importance of that input. Weights are crucial in determining how much each input influences the final output.\n",
        "\n",
        "* Summation: The perceptron calculates a weighted sum of its inputs, multiplying each input by its corresponding weight and adding them together.\n",
        "\n",
        "* Bias: A bias term is added to the weighted sum. This bias acts as an offset, allowing the perceptron to shift the activation function and make more flexible decisions.\n",
        "\n",
        "* Activation Function: The result of the summation and bias is passed through an activation function. This function introduces non-linearity, enabling the perceptron to learn complex patterns. Common activation functions include the sigmoid function, ReLU (Rectified Linear Unit), and tanh (hyperbolic tangent).\n",
        "\n",
        "* Output: The output of the activation function is the final result of the perceptron's computation. This output can be used for various tasks, such as classification or regression.\n",
        "\n",
        "![perceptron](images/perceptron.png)\n",
        "\n",
        "Now, a single perceptron has limitations in its learning capacity. It can only solve **linearly** separable problems, meaning problems where the data points can be perfectly separated by a single line or hyperplane. However, most real-world problems are not linearly separable. To overcome this limitation, we introduce the Multi-Layer Perceptron (MLP).\n",
        "\n",
        "MLPs consist of multiple layers of perceptrons, organized in an interconnected structure. MLPs overcome the limitations of single perceptrons by adding hidden layers between the input and output layers. These hidden layers allow the network to learn non-linear relationships. Each neuron in a hidden layer performs a weighted sum of its inputs, passes it through an activation function, and sends the output to the next layer. These layers include:\n",
        "\n",
        "* Input Layer: This layer receives the initial inputs to the network.\n",
        "\n",
        "* Hidden Layers: These are intermediate layers between the input and output layers. Each hidden layer contains multiple perceptrons that process the information received from the previous layer and pass their outputs to the next layer. Hidden layers enable the network to learn complex, non-linear relationships in the data.\n",
        "\n",
        "* Output Layer: This layer produces the final output of the network. The number of neurons in the output layer depends on the specific task. For example, in a binary classification problem, there would be one output neuron, while a multi-class classification problem might have multiple output neurons.\n",
        "\n",
        "![multi-layer-perceptron](images/multi-layer-perceptron.png)\n",
        "\n",
        "MLPs, with their multiple layers and non-linear activation functions, can approximate complex decision boundaries and solve problems that are not linearly separable. They are the foundation for many advanced neural network architectures and have revolutionized various fields, including image recognition, natural language processing, and robotics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Activation Functions and Backpropagation\n",
        "\n",
        "### Activation Functions\n",
        "Activation functions are crucial components of neural networks. They introduce non-linearity, enabling the network to learn complex patterns and relationships in data. Let's explore some common activation functions:\n",
        "\n",
        "* Sigmoid: The sigmoid function squashes the input to a range between 0 and 1. It's often used in binary classification problems, where the output represents the probability of belonging to a certain class. However, it suffers from the vanishing gradient problem, where gradients become very small during backpropagation, hindering learning in deep networks.   \n",
        "* Tanh (Hyperbolic Tangent): Similar to the sigmoid function, tanh squashes the input, but to a range between -1 and 1. It often performs better than sigmoid in practice, as it centers the output around 0, which can help with optimization. However, it also suffers from the vanishing gradient problem.\n",
        "* ReLU (Rectified Linear Unit): ReLU is a popular activation function that returns the input if it's positive, otherwise 0. It's computationally efficient and often leads to faster training. It also mitigates the vanishing gradient problem to some extent. However, it can suffer from the \"dying ReLU\" problem, where neurons get stuck at 0 and stop learning.\n",
        "* Leaky ReLU: Leaky ReLU is a variant of ReLU that introduces a small slope for negative inputs, preventing the dying ReLU problem. It often performs better than ReLU in practice.\n",
        "\n",
        "\n",
        "Why are ReLU-based activations often preferred?\n",
        "\n",
        "* ReLU and its variants are often preferred due to their computational efficiency and ability to mitigate the vanishing gradient problem. They generally lead to faster training and better performance in deep networks.\n",
        "\n",
        "### Backpropagation\n",
        "Backpropagation is the key algorithm for training neural networks. It allows us to efficiently calculate the gradients of the network's parameters with respect to the loss function, enabling us to update the parameters and improve the network's predictions.\n",
        "\n",
        "* Chain Rule of Calculus: Backpropagation utilizes the chain rule of calculus to compute gradients. The chain rule allows us to break down the computation of complex derivatives into smaller, more manageable steps.\n",
        "\n",
        "* Gradient Descent: The gradients calculated through backpropagation are used in gradient descent, an iterative optimization algorithm that aims to find the minimum of the loss function. The gradients indicate the direction of the steepest ascent of the loss function, and by taking steps in the opposite direction (negative gradient), we can gradually descend towards the minimum, improving the network's performance.\n",
        "\n",
        "![gradient-descent](images/gradient-descent.png)\n",
        "\n",
        "### Loss Function\n",
        "The loss function quantifies the error between the network's predictions and the actual target values. It guides the optimization process by providing a measure of how well the network is performing.\n",
        "\n",
        "* Mean Squared Error (MSE): MSE is a common loss function for regression problems. It calculates the average squared difference between the predicted and actual values.\n",
        "\n",
        "* Cross-Entropy: Cross-entropy is a popular loss function for classification problems. It measures the dissimilarity between the predicted probability distribution and the true distribution of the classes.   \n",
        "\n",
        "The choice of loss function depends on the specific learning task. The goal of backpropagation and gradient descent is to minimize the chosen loss function, leading to improved predictions and better performance on the task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a Neural Network\n",
        "\n",
        "Training a neural network involves feeding it data, adjusting its parameters to improve its predictions, and monitoring its performance. Let's explore the key concepts involved in this process:\n",
        "\n",
        "### Training Process\n",
        "* Epochs: An epoch refers to one complete pass through the entire training dataset. During each epoch, the network sees all the training examples and updates its parameters based on the errors it makes.\n",
        "\n",
        "* Batch Size: Instead of updating the parameters after every single training example, we often use batches of examples. The batch size determines how many examples are processed before updating the parameters. Smaller batch sizes can lead to more frequent updates and potentially faster convergence, but they can also introduce more noise in the training process. Larger batch sizes can be more computationally efficient but might require more memory.\n",
        "\n",
        "* Learning Rate: The learning rate controls the step size taken during gradient descent. It determines how much the parameters are adjusted based on the calculated gradients. A smaller learning rate leads to slower but potentially more stable learning, while a larger learning rate can speed up training but might risk overshooting the optimal solution.\n",
        "\n",
        "### Optimization Algorithms\n",
        "Optimization algorithms are used to update the network's parameters based on the calculated gradients. Some common algorithms include:\n",
        "\n",
        "* Stochastic Gradient Descent (SGD): SGD updates the parameters based on the gradient calculated from a single training example or a small batch of examples. It's a simple but widely used algorithm.\n",
        "\n",
        "* Adam: Adam (Adaptive Moment Estimation) is a popular optimization algorithm that combines the benefits of momentum and adaptive learning rates. It often converges faster and performs better than SGD in practice, making it a common choice for many deep learning tasks.\n",
        "\n",
        "    * Adam is often preferred due to its ability to automatically adjust learning rates for different parameters and its generally good performance across various tasks. It's relatively easy to use and often requires less hyperparameter tuning compared to other algorithms.\n",
        "\n",
        "### Overfitting/Underfitting\n",
        "* Overfitting: Overfitting occurs when the network learns the training data too well, capturing noise and irrelevant details. This leads to poor generalization performance on unseen data.\n",
        "\n",
        "* Underfitting: Underfitting happens when the network is too simple to capture the underlying patterns in the data. This results in poor performance on both training and unseen data.\n",
        "\n",
        "#### Addressing overfitting and underfitting:\n",
        "\n",
        "* Regularization: Techniques like L1 or L2 regularization add penalties to the loss function, discouraging the network from learning overly complex patterns.\n",
        "\n",
        "* Dropout: Dropout randomly drops out neurons during training, forcing the network to learn more robust features.\n",
        "\n",
        "* Early Stopping: Early stopping monitors the performance on a validation set and stops training when the performance starts to degrade, preventing the network from overfitting to the training data.\n",
        "\n",
        "* Validation Sets: A validation set is a portion of the data held out from training, used to evaluate the network's performance during training and to tune hyperparameters.\n",
        "\n",
        "\n",
        "### Visualization\n",
        "Visualizing the training process can provide insights into how the network is learning. Common visualizations include:\n",
        "\n",
        "* Loss Curve: Plotting the loss function over epochs can show how the error is decreasing during training.\n",
        "\n",
        "* Accuracy Curve: For classification tasks, plotting the accuracy on the training and validation sets can show how well the network is learning and generalizing.\n",
        "\n",
        "These visualizations help monitor the training progress, identify potential issues like overfitting or underfitting, and guide decisions about hyperparameter tuning and early stopping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Practical Example: Student Placement Classification\n",
        "\n",
        "Let's try it out with a small example.\n",
        "\n",
        "We've provided a dataset which contains information about the students academic and training and placement status.\n",
        "\n",
        "Here are the columns in the dataset:\n",
        "\n",
        "* CGPA - cumulative grade point average achieved by the student\n",
        "* Internships - number of internships a student has done\n",
        "* Projects - number of projects a student has done\n",
        "* Workshops/Certifications - number of online skills courses completed by the student\n",
        "* ApptitudeTestScore - aptitude test scores to measure the student's quantitative and logical thinking\n",
        "* SoftSkillrating - a rating of the student's communication skills\n",
        "* ExtracurricularActivities - binary (Yes/No) value indicating a student's participation in non-academic activities\n",
        "* PlacementTraining - binary (Yes/No) value indicating a student's participation in placement training\n",
        "* SSC_marks - score in senior secondary school out of 100\n",
        "* HSC_marks - score in higher secondary school out of 100\n",
        "* PlacementStatus - target variable: placed or not placed (in a job or internship)\n",
        "\n",
        "#### Process:\n",
        "1. Check Device Compatibility\n",
        "Determine whether the system has an NVIDIA GPU, AMD GPU, Apple M-series chip, or just a CPU. This helps optimize training performance.\n",
        "\n",
        "2. Load and Preprocess the Data\n",
        "    * Read the dataset into a DataFrame.\n",
        "    * Convert categorical or binary values into numerical format (e.g., mapping T/F to 1/0).\n",
        "    * Encode the target variable if necessary.\n",
        "    * Split the data into training and test sets.\n",
        "    * Normalize (scale) the feature values so that different numerical ranges don't affect training.\n",
        "3. Convert Data to PyTorch Tensors\n",
        "    * Convert the feature data (X) and target labels (y) into tensors.\n",
        "    * Move the tensors to the appropriate device (CPU or GPU).\n",
        "4. Define the Neural Network Model\n",
        "    * Create a class for the model that inherits from nn.Module.\n",
        "    * Define the layers:\n",
        "        * An input layer matching the number of features.\n",
        "        * One or more hidden layers with activation functions (e.g., ReLU).\n",
        "        * An output layer that provides a probability (using Sigmoid for binary classification).\n",
        "    * Implement the forward method to define how data flows through the layers.\n",
        "5. Set Up Loss Function and Optimizer\n",
        "    * Use Binary Cross-Entropy (BCELoss) for binary classification.\n",
        "    * Use an optimizer like Adam to adjust the model weights.\n",
        "6. Train the Model\n",
        "    * Iterate over multiple epochs:\n",
        "        * Pass the training data through the model.\n",
        "        * Compute the loss.\n",
        "        * Perform backpropagation to update weights.\n",
        "        * Print loss periodically to monitor progress.\n",
        "7. Evaluate the Model\n",
        "    * Switch the model to evaluation mode.\n",
        "    * Make predictions on the test set.\n",
        "    * Round the predictions (since outputs are probabilities).\n",
        "    * Compare predictions with actual values to compute accuracy.\n",
        "8. Compare Performance on Different Hardware\n",
        "    * Run the script on different devices (CPU, NVIDIA GPU, AMD GPU).\n",
        "    * Measure training speed and accuracy.\n",
        "    * Observe the impact of hardware on training efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hardware\n",
        "\n",
        "Pytorch can be run on a dedicated NVIDIA GPU, a dedicated AMD GPU, or on your CPU (but will be much slower).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dedicated NVIDIA GPU\n",
        "* Install CUDA: If you haven't already, install the CUDA toolkit, which provides the necessary drivers and libraries for using NVIDIA GPUs with PyTorch. You can download it from the [NVIDIA website](https://developer.nvidia.com/cuda-toolkit).\n",
        "\n",
        "* Other things I needed to do to get CUDA working:\n",
        "    * run `pip uninstall torch torchvision torchaudio`: removes the CPU only version of pytorch\n",
        "    * run `wmic path win32_VideoController get name`: find out what model GPU you have\n",
        "    * run `nvidia-smi`: find out you driver version, CUDA version, GPU model, and current GPU utilization\n",
        "    * run `pip install nvidia-pyindex --use-pep517 --no-cache-dir` or `pip install nvidia-cuda-runtime-cu12`: I had issues with pyindex so I tried these install options. Here cu12 is specific to my CUDA version.\n",
        "    * run `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128`: this is specific to my CUDA version, your link will depend on your SMI output\n",
        "\n",
        "* Run the script below to check if you have CUDA available:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "System Information:\n",
            "Python version: 3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]\n",
            "PyTorch version: 2.1.0+cu118\n",
            "CUDA available: True\n",
            "\n",
            "CUDA Details:\n",
            "CUDA version: 11.8\n",
            "Number of CUDA devices: 1\n",
            "\n",
            "Device 0 Details:\n",
            "Device name: NVIDIA RTX A3000 Laptop GPU\n",
            "Device properties: _CudaDeviceProperties(name='NVIDIA RTX A3000 Laptop GPU', major=8, minor=6, total_memory=6143MB, multi_processor_count=32)\n",
            "\n",
            "Environment Checks:\n",
            "CUDA_HOME: Not set\n",
            "PATH environment variable contains CUDA paths: True\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "\n",
        "print(\"System Information:\")\n",
        "print(\"Python version:\", sys.version)\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "try:\n",
        "    print(\"\\nCUDA Details:\")\n",
        "    print(\"CUDA version:\", torch.version.cuda)\n",
        "    print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        for i in range(torch.cuda.device_count()):\n",
        "            print(f\"\\nDevice {i} Details:\")\n",
        "            print(\"Device name:\", torch.cuda.get_device_name(i))\n",
        "            print(\"Device properties:\", torch.cuda.get_device_properties(i))\n",
        "except Exception as e:\n",
        "    print(\"Error retrieving CUDA information:\", str(e))\n",
        "\n",
        "print(\"\\nEnvironment Checks:\")\n",
        "import os\n",
        "print(\"CUDA_HOME:\", os.environ.get('CUDA_HOME', 'Not set'))\n",
        "print(\"PATH environment variable contains CUDA paths:\", \n",
        "      any('cuda' in path.lower() for path in os.environ.get('PATH', '').split(os.pathsep)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Use this code snippet to assign the model and data to your GPU:\n",
        "\n",
        "```python\n",
        "model.to(device)  # Move the model to the GPU\n",
        "\n",
        "# Inside the training loop:\n",
        "for batch_X, batch_y in train_loader:\n",
        "    batch_X = batch_X.to(device)  # Move the input data to the GPU\n",
        "    batch_y = batch_y.to(device)  # Move the target data to the GPU\n",
        "    #... rest of the training code...\n",
        "```\n",
        "\n",
        "* Compare the results of a simple computation using the GPU versus CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tensor Operations on GPU:\n",
            "X tensor: tensor([[0.3114, 0.2839, 0.6856],\n",
            "        [0.8272, 0.6210, 0.0948],\n",
            "        [0.5349, 0.1381, 0.2764],\n",
            "        [0.3981, 0.3231, 0.0982],\n",
            "        [0.6264, 0.6855, 0.2786]], device='cuda:0')\n",
            "Y tensor: tensor([[0.3130, 0.5868, 0.2942],\n",
            "        [0.3501, 0.1890, 0.3819],\n",
            "        [0.4827, 0.9265, 0.3219],\n",
            "        [0.1477, 0.3580, 0.1710],\n",
            "        [0.1217, 0.6771, 0.3953]], device='cuda:0')\n",
            "\n",
            "Addition Result: tensor([[0.6243, 0.8707, 0.9799],\n",
            "        [1.1772, 0.8101, 0.4768],\n",
            "        [1.0176, 1.0645, 0.5983],\n",
            "        [0.5458, 0.6811, 0.2692],\n",
            "        [0.7481, 1.3626, 0.6740]], device='cuda:0')\n",
            "\n",
            "Performance Comparison:\n",
            "CPU Computation Time: 0.7892224788665771\n",
            "GPU Computation Time: 0.2272329330444336\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Create a random tensor on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "x = torch.rand(5, 3, device=device)\n",
        "y = torch.rand(5, 3, device=device)\n",
        "\n",
        "print(\"Tensor Operations on GPU:\")\n",
        "print(\"X tensor:\", x)\n",
        "print(\"Y tensor:\", y)\n",
        "\n",
        "# Perform a simple GPU computation\n",
        "z = x + y\n",
        "print(\"\\nAddition Result:\", z)\n",
        "\n",
        "# Measure computation speed\n",
        "import time\n",
        "\n",
        "def cpu_computation():\n",
        "    start = time.time()\n",
        "    x_cpu = torch.rand(1000, 1000)\n",
        "    for _ in range(100):\n",
        "        x_cpu = torch.matmul(x_cpu, x_cpu)\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "def gpu_computation():\n",
        "    start = time.time()\n",
        "    x_gpu = torch.rand(1000, 1000, device='cuda')\n",
        "    for _ in range(100):\n",
        "        x_gpu = torch.matmul(x_gpu, x_gpu)\n",
        "    torch.cuda.synchronize()\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(\"CPU Computation Time:\", cpu_computation())\n",
        "print(\"GPU Computation Time:\", gpu_computation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dedicated AMD GPU\n",
        "* Install ROCm: Install the ROCm platform, which is AMD's equivalent of CUDA. You can find installation instructions on the [AMD website](https://www.amd.com/en/products/software/rocm.html).\n",
        "* Set PyTorch to use ROCm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")  # Use the default CUDA device (which will be the AMD GPU)\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Use this code (same as above) to use your GPU:\n",
        "```python\n",
        "model.to(device)  # Move the model to the GPU\n",
        "\n",
        "# Inside the training loop:\n",
        "for batch_X, batch_y in train_loader:\n",
        "    batch_X = batch_X.to(device)  # Move the input data to the GPU\n",
        "    batch_y = batch_y.to(device)  # Move the target data to the GPU\n",
        "    #... rest of the training code...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Integrated GPU/Only CPU\n",
        "\n",
        "* Check for CUDA/ROCm support: Some integrated GPUs may have limited support for CUDA or ROCm. Check the specifications of your integrated GPU and the PyTorch documentation to see if it's supported.\n",
        "* Install drivers: If your integrated GPU supports CUDA or ROCm, install the appropriate drivers and libraries.\n",
        "* Use the same code as for dedicated GPUs: If your integrated GPU is supported, you can use the same code as for dedicated GPUs to move the model and data to the GPU. However, keep in mind that integrated GPUs typically have less memory and processing power than dedicated GPUs, so training might be slower.\n",
        "\n",
        "### Considerations\n",
        "* GPU Memory: Be mindful of GPU memory limitations, especially when working with large datasets or complex models. You might need to adjust the batch size or use techniques like gradient accumulation to fit the data into GPU memory.\n",
        "* Mixed Precision Training: Consider using mixed precision training (torch.cuda.amp) to potentially speed up training on NVIDIA GPUs.\n",
        "* Multiple GPUs: If you have multiple GPUs, you can use PyTorch's nn.DataParallel or nn.DistributedDataParallel to distribute the training workload across them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practical Example: Neural Network Classification\n",
        "\n",
        "The code below trains and evaluates a neural network classifier on the placement dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using NVIDIA GPU\n",
            "Epoch [10/50], Loss: 0.5181\n",
            "Epoch [20/50], Loss: 0.4543\n",
            "Epoch [30/50], Loss: 0.4360\n",
            "Epoch [40/50], Loss: 0.4323\n",
            "Epoch [50/50], Loss: 0.4299\n",
            "Training complete!\n",
            "Accuracy: 79.30%\n",
            "Run this on different devices to compare training speed and performance.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Check GPU availability\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        device_name = torch.cuda.get_device_name(0).lower()\n",
        "        if 'nvidia' in device_name:\n",
        "            print(\"Using NVIDIA GPU\")\n",
        "        elif 'amd' in device_name:\n",
        "            print(\"Using AMD GPU\")\n",
        "        return torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        print(\"Using Apple M1/M2 GPU\")\n",
        "        return torch.device(\"mps\")\n",
        "    else:\n",
        "        print(\"Using CPU (training may be slower)\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n",
        "device = get_device()\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"data/placementdata.csv\")\n",
        "\n",
        "# Convert binary columns\n",
        "binary_columns = ['ExtracurricularActivities', 'PlacementTraining']\n",
        "for col in binary_columns:\n",
        "    data[col] = data[col].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Encode target variable\n",
        "label_encoder = LabelEncoder()\n",
        "data['PlacementStatus'] = label_encoder.fit_transform(data['PlacementStatus'])\n",
        "\n",
        "# Select features and target\n",
        "X = data.drop(columns=['PlacementStatus'])\n",
        "y = data['PlacementStatus']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Define neural network class\n",
        "class PlacementClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PlacementClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(X_train.shape[1], 16)\n",
        "        self.fc2 = nn.Linear(16, 8)\n",
        "        self.fc3 = nn.Linear(8, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Initialize model, loss, optimizer\n",
        "model = PlacementClassifier().to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_train_tensor).squeeze()\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Evaluate model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred = model(X_test_tensor).squeeze().round()\n",
        "    accuracy = (y_pred == y_test_tensor).float().mean()\n",
        "    print(f'Accuracy: {accuracy.item()*100:.2f}%')\n",
        "\n",
        "print(\"Run this on different devices to compare training speed and performance.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Next Steps:\n",
        "\n",
        "There are several ways you can experiment with this first model to better understand deep learning concepts and improve performance. \n",
        "Here are some suggestions:\n",
        "\n",
        "1. Adjust Model Architecture\n",
        "    * Increase or decrease hidden layers: Try adding more layers or making the model shallower.\n",
        "    * Change the number of neurons per layer: Test different values, such as 32, 64, or 128, to see how it affects accuracy.\n",
        "    * Try different activation functions: Replace ReLU with LeakyReLU, Tanh, or Sigmoid and compare results.\n",
        "2. Experiment with Hyperparameters\n",
        "    * Learning Rate: Try lowering it (e.g., 0.001) or increasing it (e.g., 0.1) to observe changes in convergence.\n",
        "    * Batch Size: Use mini-batch training instead of full-batch gradient descent.\n",
        "    * Optimizer Choice: Compare Adam with SGD, RMSprop, or Adagrad to see how training speed and accuracy change.\n",
        "3. Feature Engineering & Data Processing\n",
        "    * Try feature selection: Remove certain columns and check if accuracy improves or declines.\n",
        "    * Handle categorical data differently: Instead of mapping T/F to 1/0, try one-hot encoding.\n",
        "    * Experiment with different normalization techniques: Try Min-Max scaling instead of Standardization.\n",
        "4. Modify Training Strategy\n",
        "    * Increase or decrease the number of epochs: Does training for 100 epochs improve accuracy, or does it overfit?\n",
        "    * Use dropout layers: Add nn.Dropout() to prevent overfitting and observe the difference.\n",
        "    * Implement early stopping: Stop training automatically if the validation loss stops improving.\n",
        "5. Evaluate Performance in Different Ways\n",
        "    * Confusion Matrix: Visualize true positives, false positives, etc., instead of just accuracy.\n",
        "    * Precision, Recall, and F1-score: Compute these metrics to better understand model performance.\n",
        "    * Cross-validation: Instead of a single train-test split, use K-Fold cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choosing Appropriate Hyperparameters (Hyperparameter Tuning)\n",
        "Hyperparameters are settings that control the learning process of a neural network, such as the learning rate, batch size, number of hidden layers, and number of neurons per layer. Choosing appropriate hyperparameters is crucial for optimal performance.   \n",
        "\n",
        "* Manual Tuning: You can manually adjust hyperparameters based on your understanding of the model and the data. For example, if the model is overfitting, you might try reducing the learning rate or adding regularization.   \n",
        "\n",
        "* Grid Search: Grid search involves defining a set of possible values for each hyperparameter and trying all possible combinations. This can be computationally expensive but can help find a good set of hyperparameters.   \n",
        "\n",
        "* Random Search: Random search randomly samples hyperparameter values from a defined range. It can be more efficient than grid search, especially when some hyperparameters are more important than others.   \n",
        "\n",
        "* Bayesian Optimization: Bayesian optimization uses a probabilistic model to predict the performance of different hyperparameter settings and focuses on exploring promising areas of the hyperparameter space.   \n",
        "\n",
        "* Automated Hyperparameter Tuning Tools: There are tools like Optuna, Hyperopt, and Keras Tuner that automate the hyperparameter tuning process, making it easier to find optimal settings.   \n",
        "\n",
        "The choice of hyperparameter tuning method depends on the complexity of the model, the size of the dataset, and the available computational resources. It's often a good practice to start with manual tuning and then explore more automated methods if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## Neural Network Architectures\n",
        "\n",
        "While Multi-Layer Perceptrons (MLPs) are powerful, specialized architectures have emerged to efficiently handle different types of data and tasks. Let's explore some of these architectures and their motivations:\n",
        "\n",
        "* Convolutional Neural Networks (CNNs): CNNs excel at processing images and other grid-like data. They utilize convolutional kernels that act as feature detectors, sliding across the input and extracting local patterns like edges, corners, and textures. This hierarchical feature extraction makes CNNs effective for tasks like image recognition, object detection, and image segmentation.\n",
        "\n",
        "* Recurrent Neural Networks (RNNs): RNNs are designed for sequential data, such as text, time series, and speech. They have recurrent connections that allow them to maintain information about previous inputs, capturing temporal dependencies and context. This memory mechanism is crucial for tasks like language modeling, machine translation, and speech recognition.\n",
        "\n",
        "* Generative Adversarial Networks (GANs): GANs consist of two networks, a generator and a discriminator, engaged in an adversarial training process. The generator tries to create realistic data samples, while the discriminator tries to distinguish between real and generated samples. This competition pushes both networks to improve, leading to the generation of highly realistic data, such as images, videos, and audio.\n",
        "\n",
        "* Autoencoders: Autoencoders are unsupervised learning models that learn to compress and reconstruct input data. They consist of an encoder that compresses the input into a lower-dimensional representation and a decoder that reconstructs the original input from this representation. This compression and reconstruction process forces the network to learn essential features of the input data, leading to effective dimensionality reduction, anomaly detection, and feature learning.\n",
        "\n",
        "* Transformers: Transformers have revolutionized natural language processing tasks. They utilize self-attention mechanisms to capture relationships between different parts of the input sequence, weighing the importance of different parts of the input when processing information. This enables them to capture complex relationships and dependencies, making them effective for tasks like machine translation, text summarization, and question answering.\n",
        "\n",
        "### Core Concepts\n",
        "Despite the differences in architecture, the core concepts we've covered so far—activation functions, backpropagation, and optimization—remain the same across these different types of neural networks.\n",
        "\n",
        "* Activation Functions: Activation functions introduce non-linearity in all these architectures, enabling them to learn complex patterns in their respective data types.\n",
        "\n",
        "* Backpropagation: Backpropagation is used to train all these networks, calculating gradients and updating parameters to minimize the loss function.\n",
        "\n",
        "* Optimization: Optimization algorithms like SGD and Adam are used to optimize the learning process in all these architectures, guiding the networks towards better performance.\n",
        "\n",
        "Understanding these core concepts provides a solid foundation for exploring and understanding more advanced neural network architectures in the future.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Exercise: Building and Training an MLP with PyTorch\n",
        "\n",
        "*   Install PyTorch (if not already installed).\n",
        "*   Find and load a simple dataset (check out [kaggle.com](https://www.kaggle.com/)).\n",
        "*   Building a simple MLP.\n",
        "*   Define the layers (linear, activation functions) and the forward pass.\n",
        "*   Choose a loss function and an optimizer.\n",
        "*   Follow the example above with a basic training loop, forward pass, loss calculation, backpropagation, and optimization.\n",
        "*   Add a visualization to observe the training progress (loss curve)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code for basic MLP here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experimentation and Analysis\n",
        "\n",
        "*   Experiment with different hyperparameters (e.g., number of hidden layers, number of neurons per layer, learning rate, batch size, activation functions). Try one of the strategies for hyperparameter tuning in addition to manual.\n",
        "*   Observe the effects of these changes on the training process and the model's performance.\n",
        "*   Analyze your results (e.g., plot the loss curves, evaluate accuracy on a validation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Code for experimentation and analysis"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
